4.2.	Generalized Python AWS and MySQL Control Functions
All python functions in this section are developed in python version 3.6.9. Each of these functions can be implemented by typing the filename into any command line prompt and without the need to add a “python3” predecessor in the command line. The broad python commands are located inside /mnt/ss_rhb1/scratch/python, which has been added to the path so they can be called from any directory using the command line “<name of function.py> <arguments>” and contain the appropriate shebang line so there is no need to specify “python” or the specific version prior to execution.
4.2.1.	read_credentials.py
This function is utilized by most of the of the custom functions describe below and is important to mention. This function describes a custom class, Creds, and a function read_credentials which reads the instance_ids.json file. The read_credentials function returns a dictionary containing ALL of the keys in the instance_ids.json file and, therefore, contains information for all of the projects across the entire portfolio on KAAS. The project identifier is then utilized to sort the output of read_credentials to fill in the Creds class. Table 6 describes the relationship between the JSON keys (Table 3) and Creds class variables.
4.2.2.	kaas_ec2_control.py
Function to start or stop ec2 core nodes. Nodes should be started prior to any computationally intense processing that will be managed by condor. Verify the status of the core nodes by running “condor_status” and look for ip addresses beginning with “10.100.216.” Nodes should be stopped following the analyses using this same function. Command line arguments are specified in Table 7.
NOTE: this code may display an error when both starting and stopping ec2 instances; however, it should still run.
NOTE: A future upgrade should make these instances auto-scale, so as they become utilized more cores will become available and as they are completing jobs and becoming free it will scale down the cores. This should be most cost effective and efficient.
4.2.2.	kaas_ec2_control.py
Function to start or stop ec2 core nodes. Nodes should be started prior to any computationally intense processing that will be managed by condor. Verify the status of the core nodes by running “condor_status” and look for ip addresses beginning with “10.100.216.” Nodes should be stopped following the analyses using this same function. Command line arguments are specified in Table 7.
NOTE: this code may display an error when both starting and stopping ec2 instances; however, it should still run.
NOTE: A future upgrade should make these instances auto-scale, so as they become utilized more cores will become available and as they are completing jobs and becoming free it will scale down the cores. This should be most cost effective and efficient.
4.2.2.	kaas_ec2_control.py
Function to start or stop ec2 core nodes. Nodes should be started prior to any computationally intense processing that will be managed by condor. Verify the status of the core nodes by running “condor_status” and look for ip addresses beginning with “10.100.216.” Nodes should be stopped following the analyses using this same function. Command line arguments are specified in Table 7.
NOTE: this code may display an error when both starting and stopping ec2 instances; however, it should still run.
NOTE: A future upgrade should make these instances auto-scale, so as they become utilized more cores will become available and as they are completing jobs and becoming free it will scale down the cores. This should be most cost effective and efficient.
4.2.5.	kaas_s3_upload.py
Function to upload a single file or all the contents of a directory (and sub-directories) to the s3 bucket. The function inputs are detailed in Table 10. This function will, by default, copy any files uploaded to the local synchronized s3 directory. The purpose is to eliminate redundant downloads necessary for future downloads.
This function creates the custom metadata item “localmodtime” using the local disk modification time, which is a string containing the date (YYYY-MM-DD) followed by the time (HH:MM:SS), space delimited. 
To ensure efficient uploading times and to reduce redundant or unnecessary uploads, modification times between the local disk and the s3 bucket are compared to determine if the file should be uploaded. Prior to uploading, the “localmodtime” metadata item is queried. If this item exists, it is compared with the local disk modification time and the file is uploaded if the s3 bucket file is older than the file on disk. If this metadata item does not exist, the s3 bucket last_modification tag is compared against the local disk modification time. If the s3 bucket file is older than the disk on file, the file is uploaded.
4.2.6.	kaas_syncs3.py
Function to synchronize files from the s3 bucket to the local disk. The function inputs are detailed in Table 11.
